<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>决策树</title>
</head>
<body>
<div>
	决策树
</div>
<script type="text/javascript">
// 决策树
// 决策树是一种常见的机器学习算法，用于分类和回归问题。下面是使用JavaScript实现决策树的示例代码：
class DecisionTree {
  constructor(data, target, features) {
    this.data = data;
    this.target = target;
    this.features = features;
    this.tree = this.buildTree(data, target, features);
  }

  buildTree(data, target, features) {
    // 确定终止条件
    if (this.isHomogeneous(target)) {
      return { label: target[0] };
    }
    if (features.length === 0) {
      return { label: this.majorityVote(target) };
    }

    // 选择最佳分裂点
    const bestFeature = this.getBestFeature(data, target, features);
    const remainingFeatures = features.filter(feature => feature !== bestFeature);
    const tree = { feature: bestFeature, children: [] };

    // 根据最佳分裂点创建子节点
    const featureValues = [...new Set(data.map(row => row[bestFeature]))];
    for (const value of featureValues) {
      const subset = data.filter(row => row[bestFeature] === value);
      const subtree = this.buildTree(subset, subset.map(row => row[target]), remainingFeatures);
      tree.children.push({ value, subtree });
    }

    return tree;
  }

  predict(sample) {
    let currentNode = this.tree;
    while (currentNode.children) {
      const featureValue = sample[currentNode.feature];
      const childNode = currentNode.children.find(child => child.value === featureValue);
      currentNode = childNode.subtree;
    }
    return currentNode.label;
  }

  isHomogeneous(array) {
    return new Set(array).size === 1;
  }

  majorityVote(array) {
    const counts = {};
    for (const item of array) {
      if (counts[item]) {
        counts[item]++;
      } else {
        counts[item] = 1;
      }
    }
    return Object.keys(counts).reduce((a, b) => counts[a] > counts[b] ? a : b);
  }

  getBestFeature(data, target, features) {
    let bestFeature;
    let bestInformationGain = -Infinity;
    for (const feature of features) {
      const informationGain = this.calculateInformationGain(data, target, feature);
      if (informationGain > bestInformationGain) {
        bestFeature = feature;
        bestInformationGain = informationGain;
      }
    }
    return bestFeature;
  }

  calculateInformationGain(data, target, feature) {
    const featureValues = [...new Set(data.map(row => row[feature]))];
    const weightedEntropy = featureValues.reduce((sum, value) => {
      const subset = data.filter(row => row[feature] === value);
      const subsetTarget = subset.map(row => row[target]);
      const proportion = subset.length / data.length;
      return sum + proportion * this.calculateEntropy(subsetTarget);
    }, 0);
    return this.calculateEntropy(target) - weightedEntropy;
  }

  calculateEntropy(array) {
    const counts = {};
    for (const item of array) {
      if (counts[item]) {
        counts[item]++;
      } else {
        counts[item] = 1;
      }
    }
    const proportions = Object.values(counts).map(count => count / array.length);
    return proportions.reduce((sum, p) => sum - p * Math.log2(p), 0);
  }
}

/*上面的代码定义了一个DecisionTree类，构造函数接受三个参数：数据data、目标变量target和特征列表features。使用这三个参数，我们可以通过调用buildTree方法构建一棵决策树。buildTree方法使用递归的方式，首先确定终止条件（数据集中的所有目标变量值都相同或没有可用特征），然后选择最佳分裂点（具有最大信息增益的特征），并创建子节点。最后返回整棵树的根节点。

predict方法接受一个样本sample作为参数，并使用训练好的决策树预测该样本的目标变量值。该方法使用当前节点的特征和子节点的值，沿着树向下遍历，直到到达叶子节点，然后返回叶子节点的目标变量值。

isHomogeneous方法检查数组中的所有元素是否相同，如果是，则说明该数组是同质的。

majorityVote方法接受一个数组作为参数，并返回该数组中出现最频繁的元素。

getBestFeature方法接受数据集、目标变量和特征列表作为参数，并使用信息增益选择最佳分裂点。

calculateInformationGain方法接受数据集、目标变量和特征作为参数，并计算使用该特征分裂时的信息增益。

calculateEntropy方法接受一个数组作为参数，并计算该数组的熵。*/


/*由于医疗数据通常属于敏感数据，需要严格保护隐私，因此我们无法提供实际的医疗数据集示例。但是，我们可以提供一个示例代码，用于演示如何使用上面的决策树算法对数据进行分类。

假设我们有一个糖尿病数据集，其中包含了病人的一些特征信息和是否患有糖尿病的标签。我们将使用该数据集来训练一个决策树模型，并使用该模型对新的患者进行预测。

首先，我们需要准备数据集并将其划分为训练集和测试集。以下是一个简单的示例代码：*/

// 导入数据集
const diabetesData = [
  [6, 148, 72, 35, 0, 33.6, 0.627, 50, 1],
  [1, 85, 66, 29, 0, 26.6, 0.351, 31, 0],
  [8, 183, 64, 0, 0, 23.3, 0.672, 32, 1],
  // ... 其他样本数据
];

// 划分训练集和测试集
const [trainData, testData] = splitData(diabetesData, 0.8);

// 接下来，我们可以使用上面实现的决策树算法训练一个模型，并使用该模型对测试集进行预测。以下是一个示例代码：

// 训练模型
const features = ['pregnancies', 'glucose', 'bloodPressure', 'skinThickness', 'insulin', 'bmi', 'diabetesPedigreeFunction', 'age'];
const target = 'diabetes';
const dt = new DecisionTree(trainData, target, features);
dt.buildTree();

// 对测试集进行预测
let correctCount = 0;
for (const sample of testData) {
  const predictedLabel = dt.predict(sample);
  const actualLabel = sample[target];
  if (predictedLabel === actualLabel) {
    correctCount++;
  }
}

const accuracy = correctCount / testData.length;
console.log('Accuracy:', accuracy);


// 在这个示例中，我们使用了上面实现的DecisionTree类，使用数据集的特征、目标变量和特征列表来创建一个决策树，并使用buildTree方法训练该模型。然后，我们使用predict方法对测试集进行预测，并计算准确率。

// 需要注意的是，这里的准确率仅是一个简单的指标，实际上在医疗领域中，需要更加详细和全面的指标来评估模型的性能，例如召回率、精确率和F1值等指标。







//--------------以下是重新让chatGPT生成了注释版决策树-----------

/**
 * 决策树节点类
 */
class Node {
  /**
   * 构造函数
   * @param {Object} data 节点数据，包括特征名称、特征值、样本列表和子节点列表
   */
  constructor(data) {
    this.feature = data.feature; // 特征名称
    this.value = data.value; // 特征值
    this.samples = data.samples; // 样本列表
    this.children = data.children || []; // 子节点列表
  }

  /**
   * 添加子节点
   * @param {Node} child 子节点
   */
  addChild(child) {
    this.children.push(child);
  }
}

/**
 * 决策树类
 */
class DecisionTree {
  /**
   * 构造函数
   * @param {Array} data 数据集，包括特征和目标变量
   * @param {string} target 目标变量名称
   * @param {Array} features 特征列表
   * @param {number} minSamplesSplit 最小样本数，小于该值时不再分裂节点
   * @param {number} maxDepth 最大深度，达到该深度时停止分裂节点
   */
  constructor(data, target, features, minSamplesSplit = 2, maxDepth = Infinity) {
    this.data = data;
    this.target = target;
    this.features = features;
    this.minSamplesSplit = minSamplesSplit;
    this.maxDepth = maxDepth;
    this.root = null; // 根节点
  }

  /**
   * 计算基尼不纯度
   * @param {Array} samples 样本列表
   * @param {string} target 目标变量名称
   * @returns {number} 基尼不纯度
   */
  _calcGini(samples, target) {
    const classCounts = {};
    for (const sample of samples) {
      const label = sample[target];
      if (label in classCounts) {
        classCounts[label]++;
      } else {
        classCounts[label] = 1;
      }
    }
    let gini = 1;
    for (const label in classCounts) {
      const probability = classCounts[label] / samples.length;
      gini -= probability ** 2;
    }
    return gini;
  }

  /**
   * 划分数据集
   * @param {Array} samples 样本列表
   * @param {string} feature 特征名称
   * @param {any} value 特征值
   * @returns {Array} 划分后的两个子集
   */
  _splitData(samples, feature, value) {
    const left = [];
    const right = [];
    for (const sample of samples) {
      if (sample[feature] === value) {
        left.push(sample);
      } else {
        right.push(sample);
      }
    }
    return [left, right];
  }

  /**
 * 计算划分后的基尼不纯度
 * @param {Array} left 左子集样本列表
 * @param {Array} right 右子集样本列表
 * @param {string} target 目标变量名称
 * @returns {number} 划分后的基尼不纯度
 */
  _calcSplitGini(left, right, target) {
    const n = left.length + right.length;
    const leftGini = this._calcGini(left, target);
    const rightGini = this._calcGini(right, target);
    return (left.length / n) * leftGini + (right.length / n) * rightGini;
  }

  /**
   * 选择最佳划分特征和特征值
   * @param {Array} samples 样本列表
   * @param {Array} features 特征列表
   * @param {string} target 目标变量名称
   * @returns {Object} 最佳划分特征和特征值
   */
  _chooseBestSplit(samples, features, target) {
    let bestFeature = null;
    let bestValue = null;
    let bestGini = Infinity;
    for (const feature of features) {
      const featureValues = new Set(samples.map((sample) => sample[feature]));
      for (const value of featureValues) {
        const [left, right] = this._splitData(samples, feature, value);
        if (left.length >= this.minSamplesSplit && right.length >= this.minSamplesSplit) {
          const gini = this._calcSplitGini(left, right, target);
          if (gini < bestGini) {
            bestFeature = feature;
            bestValue = value;
            bestGini = gini;
          }
        }
      }
    }
    return { feature: bestFeature, value: bestValue };
  }

  /**
   * 构建决策树
   * @param {Node} node 当前节点
   * @param {number} depth 当前深度
   */
  _buildTree(node, depth) {
    const samples = node.samples;
    const targetValues = new Set(samples.map((sample) => sample[this.target]));
    // 如果所有样本的目标变量相同，则不再分裂节点，将当前节点作为叶子节点
    if (targetValues.size === 1) {
      node.label = samples[0][this.target];
      return;
    }
        // 如果特征列表非空，选择最佳划分特征和特征值，并按照最佳划分特征和特征值分裂节点，递归构建子树
    const { feature, value } = this._chooseBestSplit(samples, this.features, this.target);
    if (!feature) {
      const classCounts = {};
      for (const sample of samples) {
        const label = sample[this.target];
        if (label in classCounts) {
          classCounts[label]++;
        } else {
          classCounts[label] = 1;
        }
      }
      let maxCount = -Infinity;
      for (const label in classCounts) {
        if (classCounts[label] > maxCount) {
          maxCount = classCounts[label];
          node.label = label;
        }
      }
      return;
    }
    const [left, right] = this._splitData(samples, feature, value);
    node.feature = feature;
    node.value = value;
    node.left = new Node(left);
    node.right = new Node(right);
    this.features = this.features.filter((f) => f !== feature);
    this._buildTree(node.left, depth + 1);
    this._buildTree(node.right, depth + 1);
  }

  /**
   * 预测单个样本的标签
   * @param {Object} sample 样本
   * @param {Node} node 当前节点
   * @returns {string} 标签
   */
  _predictSample(sample, node) {
    if (node.label !== null) {
      return node.label;
    }
    if (sample[node.feature] < node.value) {
      return this._predictSample(sample, node.left);
    } else {
      return this._predictSample(sample, node.right);
    }
  }

  /**
   * 预测多个样本的标签
   * @param {Array} samples 样本列表
   * @returns {Array} 标签列表
   */
  predict(samples) {
    return samples.map((sample) => this._predictSample(sample, this.root));
  }
}

/**
 * 决策树节点类
 */
class Node {
  constructor(samples) {
    this.samples = samples; // 节点所包含的样本列表
    this.feature = null; // 分裂特征名称
    this.value = null; // 分裂特征值
    this.left = null; // 左子树
    this.right = null; // 右子树
    this.label = null; // 叶子节点的标签
  }
}

// 示例用法
const data = [
  { age: 20, gender: 'male', label: 'A' },
  { age: 25, gender: 'female', label: 'B' },
  { age: 30, gender: 'male', label: 'B' },
  { age: 35, gender: 'female', label: 'A' },
  { age: 40, gender: 'male', label: 'B' },
  { age: 45, gender: 'female', label: 'A' },
];
const tree = new DecisionTree({
  minSamplesSplit: 2,
  maxDepth: 2,
  features: ['age', 'gender'],
  target: 'label',
});
tree.fit(data);
console.log(tree.predict([{ age: 22, gender


</script>
</body>
</html>